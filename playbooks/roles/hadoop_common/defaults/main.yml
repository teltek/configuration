---
#
# edX Configuration
#
# github:     https://github.com/edx/configuration
# wiki:       https://openedx.atlassian.net/wiki/display/OpenOPS
# code style: https://openedx.atlassian.net/wiki/display/OpenOPS/Ansible+Code+Conventions
# license:    https://github.com/edx/configuration/blob/master/LICENSE.TXT
#
##
# Defaults for role hadoop_common
#

HADOOP_COMMON_VERSION: 2.3.0
HADOOP_COMMON_USER_HOME: "{{ COMMON_APP_DIR }}/hadoop"
HADOOP_COMMON_HOME: "{{ HADOOP_COMMON_USER_HOME }}/hadoop"
HADOOP_COMMON_DATA: "{{ COMMON_DATA_DIR }}/hadoop"

# These are non-standard directories, but are where Hadoop expects to find them.
HADOOP_COMMON_LOGS: "{{ HADOOP_COMMON_HOME }}/logs"
HADOOP_COMMON_CONF_DIR: "{{ HADOOP_COMMON_HOME }}/etc/hadoop"

HADOOP_COMMON_PROTOBUF_VERSION: 2.5.0
HADOOP_COMMON_SERVICES_DIR: "{{ HADOOP_COMMON_USER_HOME }}/services.d"
# http://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-daemons.html
# c3.xlarge
HADOOP_COMMON_SERVICE_HEAP_MAX: 2252
HADOOP_COMMON_TOOL_HEAP_MAX: 1024

hadoop_common_role_name: hadoop_common
hadoop_common_user: hadoop
hadoop_common_group: hadoop
hadoop_common_temporary_dir: /var/tmp
hadoop_common_dist:
  filename: "hadoop-{{ HADOOP_COMMON_VERSION }}.tar.gz"
  url: "https://archive.apache.org/dist/hadoop/core/hadoop-{{ HADOOP_COMMON_VERSION }}/hadoop-{{ HADOOP_COMMON_VERSION }}.tar.gz"
  sha256sum: 3fad58b525a47cf74458d0996564a2151c5a28baa1f92383e7932774deef5023
hadoop_common_protobuf_dist:
  filename: "protobuf-{{ HADOOP_COMMON_PROTOBUF_VERSION }}.tar.gz"
  url: "https://github.com/google/protobuf/releases/download/v{{ HADOOP_COMMON_PROTOBUF_VERSION }}/protobuf-{{ HADOOP_COMMON_PROTOBUF_VERSION }}.tar.gz"
  sha256sum: c55aa3dc538e6fd5eaf732f4eb6b98bdcb7cedb5b91d3b5bdcf29c98c293f58e
hadoop_common_native_dist:
  filename: "release-{{ HADOOP_COMMON_VERSION }}.tar.gz"
  url: "https://github.com/apache/hadoop-common/archive/release-{{ HADOOP_COMMON_VERSION }}.tar.gz"
  sha256sum: 19d6084f7f7f491c84e345b45c8d95f89a097d04bdcd0705d40657b82977d248
hadoop_common_java_home: "{{ oraclejdk_link }}"
hadoop_common_env: "{{ HADOOP_COMMON_HOME }}/hadoop_env"

#
# OS packages
#

hadoop_common_debian_pkgs:
  - gcc
  - build-essential
  - make
  - cmake
  - automake
  - autoconf
  - libtool
  - zlib1g-dev
  - maven

hadoop_common_redhat_pkgs: []

#
# MapReduce/Yarn memory config (defaults for m1.medium)
# http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/TaskConfiguration_H2.html
#
# mapred_site_config:
#   mapreduce.map.memory_mb: 768
#   mapreduce.map.java.opts: '-Xmx512M'
#   mapreduce.reduce.memory.mb: 1024
#   mapreduce.reduce.java.opts: '-Xmx768M'

# yarn_site_config:
#   yarn.app.mapreduce.am.resource.mb: 1024
#   yarn.scheduler.minimum-allocation-mb: 32
#   yarn.scheduler.maximum-allocation-mb: 2048
#   yarn.nodemanager.resource.memory-mb: 2048
#   yarn.nodemanager.vmem-pmem-ratio: 2.1

mapred_site_config:
  mapreduce.map.memory.mb: 3072
  mapreduce.map.java.opts: '-Xmx2458m'
  mapreduce.reduce.memory.mb: 6144
  mapreduce.reduce.java.opts: '-Xmx4916m'

yarn_site_config:
  yarn.log-aggregation-enable: true
  # 24 hour log retention
  yarn.log-aggregation.retain-seconds: 86400
  # Checking virtual memory usage causes too many spurious failures.
  yarn.nodemanager.vmem-check-enabled: false
  yarn.app.mapreduce.am.resource.mb: 6144
  yarn.scheduler.minimum-allocation-mb: 32
  yarn.scheduler.maximum-allocation-mb: 6144
  yarn.nodemanager.resource.memory-mb: 6144

HADOOP_CORE_SITE_EXTRA_CONFIG: {}
HDFS_SITE_EXTRA_CONFIG: {}

# Define all the services here, assuming we have a single-node Hadoop
# cluster.
hadoop_common_services:
  - hdfs-namenode
  - hdfs-datanode
  - yarn-resourcemanager
  - yarn-nodemanager
  - yarn-proxyserver
  - mapreduce-historyserver
